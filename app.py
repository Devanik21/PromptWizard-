# This is the updated code with added functionalities for the specified libraries and a new Prompt Workflow tab.

import streamlit as st
import google.generativeai as genai
import pandas as pd
import time
import json
import os
from datetime import datetime
import pyperclip

# Libraries to be added/demonstrated
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from textblob import TextBlob
# Removed nltk import and related imports
# import nltk
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize
from gtts import gTTS
from PIL import Image
import requests
from io import BytesIO

# Removed NLTK download code block
# try:
#     nltk.data.find('tokenizers/punkt')
# except nltk.downloader.DownloadError:
#     nltk.download('punkt')
# except LookupError:
#     nltk.download('punkt')

# try:
#     nltk.data.find('corpora/stopwords')
# except nltk.downloader.DownloadError:
#     nltk.download('stopwords')
# except LookupError:
#     nltk.download('stopwords')



st.set_page_config(page_title="Prompt Engineer's Toolkit", layout="centered",page_icon="üõ†Ô∏è")
st.title("üîß Prompt Engineer's Toolkit")
st.markdown("""
Enter a **goal** and optionally a **poor prompt**. This tool will generate optimized prompt templates and debug the poor one.
Added functionalities for data visualization, text processing (using TextBlob), image handling, web requests, and a Prompt Workflow builder are also included.
""")

st.sidebar.title("üîê API Configuration")
api_key = st.sidebar.text_input("Enter your Gemini API Key", type="password")
model_option = st.sidebar.selectbox("Select Gemini Model", ["models/gemini-2.0-flash", "models/gemini-2.0-pro", "models/gemini-1.5-flash"])
temperature = st.sidebar.slider("Temperature", 0.0, 1.0, 0.7, 0.1)

if st.sidebar.checkbox("Enable Prompt History"):
    if not os.path.exists("prompt_history"):
        os.makedirs("prompt_history")

theme = st.sidebar.radio("Theme", ["Light", "Dark"])
if theme == "Dark":
    st.markdown("""
    <style>
    .stApp {
        background-color: #121212;
        color: white;
    }
    </style>
    """, unsafe_allow_html=True)

if api_key:
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(model_option, generation_config={"temperature": temperature})

    goal = st.text_area("üéØ Your Goal", placeholder="E.g. Summarize academic research papers in a friendly tone", height=100)

    examples = {
        "": "Custom Goal",
        "academic_summary": "Summarize academic research papers in a friendly tone",
        "data_analysis": "Create detailed data analysis prompts for complex datasets",
        "creative_story": "Generate creative short stories based on a theme",
        "code_explanation": "Explain complex code in simple terms for beginners"
    }
    example_choice = st.selectbox("Or choose from example goals:", options=list(examples.keys()))
    if example_choice and example_choice != "":
        goal = examples[example_choice]

    poor_prompt = st.text_area("üêå Poor Prompt (optional)", placeholder="E.g. summarize this", height=100)

    with st.expander("Advanced Options"):
        output_format = st.radio("Output Format", ["Text", "JSON", "Markdown"])
        num_prompts = st.slider("Number of prompts to generate", 1, 5, 3)

    # Added a new tab for Prompt Workflow
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["Generate Prompts", "A/B Test Prompts", "üí¨ Prompt Coach Chat", "üî¨ Library Demonstrations", "üîó Prompt Workflow"])

    with tab1:
        if st.button("Generate Optimized Prompts") and goal:
            with st.spinner("Crafting prompt magic ‚ú®"):
                prompt_format = output_format.lower()
                prompt = f"""
You are an expert prompt engineer. Given the goal:
'{goal}'
1. Suggest {num_prompts} optimized prompts that would achieve this goal.
2. If a poor prompt is provided, critique it and rewrite it.
Poor Prompt (if any): '{poor_prompt}'
Please format your response in {prompt_format} format.
"""
                start_time = time.time()
                response = model.generate_content(prompt)
                end_time = time.time()

                st.subheader("üß† Gemini's Output")
                st.markdown(response.text)
                st.info(f"Response generated in {end_time - start_time:.2f} seconds")

                if st.sidebar.checkbox("Enable Prompt History", key="save_history"):
                    now = datetime.now().strftime("%Y%m%d_%H%M%S")
                    with open(f"prompt_history/prompt_{now}.json", "w") as f:
                        json.dump({
                            "goal": goal,
                            "poor_prompt": poor_prompt,
                            "response": response.text,
                            "model": model_option,
                            "temperature": temperature
                        }, f)

                if st.button("Copy to Clipboard"):
                    pyperclip.copy(response.text)
                    st.success("Copied to clipboard!")

        with st.expander("üìà Prompt Quality Analyzer"):
            analyze_prompt = st.text_area("Enter a prompt to evaluate", height=100)
            if st.button("Analyze Quality"):
                score_prompt = f"""
You are an expert prompt engineer. Score the following prompt on:
- Clarity (0-10)
- Completeness (0-10)
- Tone (0-10)
- Effectiveness (0-10)
Then suggest improvements.
Prompt: {analyze_prompt}
"""
                analysis = model.generate_content(score_prompt)
                st.markdown(analysis.text)

        with st.expander("üß™ Prompt Sandbox"):
            sandbox_prompt = st.text_area("Prompt", height=100)
            sandbox_input = st.text_area("Input", height=100)
            if st.button("Run in Sandbox"):
                sandbox_result = model.generate_content(f"{sandbox_prompt}\n\n{sandbox_input}")
                st.markdown("### Result")
                st.markdown(sandbox_result.text)

        with st.expander("üé° Generate Random Prompt Ideas"):
            idea_topic = st.text_input("Topic (e.g., marketing, education, writing)")
            if st.button("Get Prompt Ideas"):
                idea_response = model.generate_content(f"Generate 5 creative prompt ideas about: {idea_topic}")
                st.markdown(idea_response.text)

        with st.expander("üé® Style Transformer"):
            base_prompt = st.text_area("Original Prompt")
            style = st.selectbox("Select Style", ["Friendly", "Professional", "Poetic", "Sarcastic"])
            if st.button("Transform Style"):
                styled_response = model.generate_content(f"Rewrite this prompt in a {style} tone:\n{base_prompt}")
                st.markdown(styled_response.text)

        with st.expander("üìö Prompt Template Library"):
            templates = {
                "Explain Code": "Explain the following code to a beginner:\n{code}",
                "Rewrite Article": "Rewrite this article for kids:\n{text}",
                "Keyword Extractor": "Extract main keywords from:\n{content}"
            }
            template_selected = st.selectbox("Choose Template", list(templates.keys()))
            st.code(templates[template_selected])

        with st.expander("üåç Multilingual Prompt Helper"):
            original_prompt = st.text_area("Prompt in English")
            target_lang = st.selectbox("Translate to", ["Spanish", "French", "Hindi", "Japanese"])
            if st.button("Translate Prompt"):
                translated = model.generate_content(f"Translate to {target_lang}:\n{original_prompt}")
                st.markdown(translated.text)

        with st.expander("‚ö†Ô∏è Prompt Risk Checker"):
            check_prompt = st.text_area("Prompt to check", height=100)
            if st.button("Check Risk"):
                risk_result = model.generate_content(f"Analyze this prompt for sensitive content:\n{check_prompt}")
                st.markdown(risk_result.text)

        with st.expander("üß† Prompt Use Case Generator"):
            theme_input = st.text_input("Enter a theme or domain")
            if st.button("Generate Use Cases"):
                use_cases = model.generate_content(f"Generate 5 prompt use cases for: {theme_input}")
                st.markdown(use_cases.text)

        with st.expander("üìä Prompt to Slide Generator"):
            slide_prompt = st.text_area("Enter topic for slides")
            if st.button("Generate Slides Outline"):
                slides = model.generate_content(f"Generate slide deck outline for: {slide_prompt}")
                st.markdown(slides.text)

        with st.expander("üßµ Thread Generator for X/Twitter"):
            thread_topic = st.text_input("Topic for Twitter Thread")
            if st.button("Generate Thread"):
                thread = model.generate_content(f"Generate a Twitter thread on: {thread_topic}")
                st.markdown(thread.text)

        with st.expander("üß© Reverse Prompt Engineer"):
            reverse_prompt = st.text_area("Output you want", height=100)
            if st.button("Find Possible Prompt"):
                reverse = model.generate_content(f"What prompt would lead to this output:\n{reverse_prompt}")
                st.markdown(reverse.text)

        with st.expander("üí° Prompt Variants Explorer"):
            variant_base = st.text_area("Enter base prompt")
            if st.button("Generate Variants"):
                variants = model.generate_content(f"Generate 5 creative variants of: {variant_base}")
                st.markdown(variants.text)

        with st.expander("üîÑ Prompt Reformatter"):
            messy_prompt = st.text_area("Enter poorly structured prompt")
            if st.button("Reformat Prompt"):
                cleaned = model.generate_content(f"Reformat this prompt clearly and professionally:\n{messy_prompt}")
                st.markdown(cleaned.text)

        with st.expander("üóÉÔ∏è Prompt Storage Suggestor"):
            storage_type = st.text_input("Describe your use case")
            if st.button("Suggest Storage"):
                suggestion = model.generate_content(f"Suggest best way to store prompts for this use: {storage_type}")
                st.markdown(suggestion.text)

        with st.expander("üîç SEO Prompt Optimizer"):
            seo_prompt = st.text_area("Enter content-related prompt")
            if st.button("Optimize for SEO"):
                optimized = model.generate_content(f"Optimize this prompt for SEO content creation:\n{seo_prompt}")
                st.markdown(optimized.text)

        with st.expander("üß† Few-Shot Prompt Creator"):
            task_desc = st.text_area("Task Description")
            examples_few = st.text_area("Examples (comma separated)")
            if st.button("Generate Few-Shot Prompt"):
                few_shot = model.generate_content(f"Create few-shot prompt for: {task_desc} using examples: {examples_few}")
                st.markdown(few_shot.text)




    with tab2:
        st.write("Compare two different prompts to see which performs better")
        prompt_a = st.text_area("Prompt A", height=100)
        prompt_b = st.text_area("Prompt B", height=100)
        test_input = st.text_area("Test Input", placeholder="Enter text to test these prompts on", height=100)

        if st.button("Run A/B Test") and prompt_a and prompt_b and test_input:
            with st.spinner("Testing prompts..."):
                response_a = model.generate_content(f"{prompt_a}\n\n{test_input}")
                response_b = model.generate_content(f"{prompt_b}\n\n{test_input}")

                col1, col2 = st.columns(2)
                with col1:
                    st.subheader("Result A")
                    st.markdown(response_a.text)

                with col2:
                    st.subheader("Result B")
                    st.markdown(response_b.text)

                if st.button("Export Results"):
                    results = {
                        "prompt_a": prompt_a,
                        "prompt_b": prompt_b,
                        "test_input": test_input,
                        "result_a": response_a.text,
                        "result_b": response_b.text,
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                    df = pd.DataFrame([results])
                    csv = df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        label="Download CSV",
                        data=csv,
                        file_name=f"ab_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )

    with tab3:
        if "chat_history" not in st.session_state:
            st.session_state.chat_history = []

        user_input = st.text_input("You:", key="chat_input")
        if st.button("Send"):
            st.session_state.chat_history.append(("You", user_input))
            coach_reply = model.generate_content(f"You are a prompt engineering coach. Help with: {user_input}")
            st.session_state.chat_history.append(("Coach", coach_reply.text))

        for sender, msg in st.session_state.chat_history:
            st.markdown(f"**{sender}:** {msg}")

    # Tab for Library Demonstrations
    with tab4:
        st.header("üî¨ Library Demonstrations")

        with st.expander("üìä Plotly & Matplotlib/Seaborn Demonstration"):
            st.subheader("Data Visualization")
            st.write("Generate plots using Plotly, Matplotlib, and Seaborn.")

            # Example Data for plotting
            data = {'Category': ['A', 'B', 'C', 'D', 'E'],
                    'Value': [10, 15, 7, 12, 9]}
            df = pd.DataFrame(data)

            st.write("Sample Data:")
            st.dataframe(df)

            # Plotly Example (Interactive)
            st.subheader("Plotly Chart (Interactive)")
            fig_plotly = px.bar(df, x='Category', y='Value', title='Plotly Bar Chart')
            st.plotly_chart(fig_plotly)

            # Matplotlib Example (Static)
            st.subheader("Matplotlib Chart (Static)")
            fig_mpl, ax = plt.subplots()
            ax.bar(df['Category'], df['Value'])
            ax.set_title('Matplotlib Bar Chart')
            ax.set_xlabel('Category')
            ax.set_ylabel('Value')
            st.pyplot(fig_mpl)

            # Seaborn Example (Static)
            st.subheader("Seaborn Chart (Static)")
            fig_seaborn, ax_seaborn = plt.subplots()
            sns.barplot(x='Category', y='Value', data=df, ax=ax_seaborn)
            ax_seaborn.set_title('Seaborn Bar Chart')
            st.pyplot(fig_seaborn)

        with st.expander("‚òÅÔ∏è Wordcloud Demonstration"):
            st.subheader("Wordcloud Generation")
            st.write("Generate a wordcloud from text input.")
            wordcloud_text = st.text_area("Enter text for Wordcloud", "Enter your text here to generate a wordcloud demonstration. Wordcloud is a great way to visualize text data frequency.", height=100)
            if st.button("Generate Wordcloud"):
                if wordcloud_text:
                    wc = WordCloud(width=800, height=400, background_color='white').generate(wordcloud_text)
                    plt.figure(figsize=(10, 5))
                    plt.imshow(wc, interpolation='bilinear')
                    plt.axis("off")
                    st.pyplot(plt)
                else:
                    st.warning("Please enter some text to generate a wordcloud.")

        with st.expander("üìù TextBlob Demonstration"):
            st.subheader("Text Analysis (TextBlob)")
            st.write("Perform sentiment analysis using TextBlob.")
            analysis_text = st.text_area("Enter text for analysis", "TextBlob is great for sentiment analysis.", height=100)
            if st.button("Analyze Text"):
                if analysis_text:
                    # TextBlob Sentiment
                    blob = TextBlob(analysis_text)
                    st.write(f"Sentiment Polarity: {blob.sentiment.polarity:.2f}")
                    st.write(f"Sentiment Subjectivity: {blob.sentiment.subjectivity:.2f}")
                else:
                    st.warning("Please enter some text for analysis.")

        with st.expander("üó£Ô∏è gTTS Demonstration"):
            st.subheader("Text-to-Speech")
            st.write("Convert text to speech using Google Text-to-Speech.")
            tts_text = st.text_area("Enter text to convert to speech", "Hello, this is a text to speech demonstration using gTTS.", height=100)
            # Using a unique key for the button
            if st.button("Generate Speech", key="generate_speech_button"):
                if tts_text:
                    try:
                        tts = gTTS(text=tts_text, lang='en', slow=False)
                        # Save to a bytes object instead of a file
                        audio_bytes_io = BytesIO()
                        tts.write_to_fp(audio_bytes_io)
                        audio_bytes_io.seek(0)
                        st.audio(audio_bytes_io, format="audio/mp3")
                    except Exception as e:
                        st.error(f"Error generating speech: {e}")
                else:
                    st.warning("Please enter some text to convert to speech.")

        with st.expander("üñºÔ∏è Pillow Demonstration"):
            st.subheader("Image Handling")
            st.write("Display an uploaded image using Pillow.")
            uploaded_image = st.file_uploader("Upload an image", type=["jpg", "png", "jpeg"])
            if uploaded_image is not None:
                try:
                    img = Image.open(uploaded_image)
                    st.image(img, caption="Uploaded Image", use_column_width=True)
                except Exception as e:
                    st.error(f"Error loading image: {e}")

        with st.expander("üåê Requests Demonstration"):
            st.subheader("Make HTTP Requests")
            st.write("Fetch data from a URL using the requests library.")
            request_url = st.text_input("Enter a URL", "https://www.google.com")
            if st.button("Fetch URL Content"):
                if request_url:
                    try:
                        response = requests.get(request_url)
                        st.write(f"Status Code: {response.status_code}")
                        st.write("Partial Content (first 500 characters):")
                        st.text(response.text[:500] + "...")
                    except Exception as e:
                        st.error(f"Error fetching URL: {e}")
                else:
                    st.warning("Please enter a URL.")

    # New tab for Prompt Workflow
    with tab5:
        st.header("üîó Prompt Workflow Builder")
        st.write("Build a sequence of prompts where the output of one becomes the input for the next.")

        # Initialize prompt steps in session state if not already present
        if 'prompt_workflow_steps' not in st.session_state:
            st.session_state.prompt_workflow_steps = [{}]

        initial_input = st.text_area("Initial Input for the Workflow", height=150, key='workflow_initial_input')

        st.subheader("Define Prompt Steps:")

        # Display existing prompt steps and allow editing
        for i in range(len(st.session_state.prompt_workflow_steps)):
            step = st.session_state.prompt_workflow_steps[i]
            st.markdown(f"**Step {i + 1}**")
            step['prompt'] = st.text_area(f"Prompt for Step {i + 1}", height=100, key=f'workflow_prompt_{i}')
            if i > 0:
                # Add a button to remove steps (except the first one)
                if st.button(f"Remove Step {i + 1}", key=f'remove_workflow_step_{i}'):
                    st.session_state.prompt_workflow_steps.pop(i)
                    st.rerun() # Rerun to update the UI after removing a step

        # Button to add a new prompt step
        if st.button("Add Another Step"):
            st.session_state.prompt_workflow_steps.append({})
            st.rerun() # Rerun to show the new text area

        if st.button("Run Workflow") and st.session_state.prompt_workflow_steps:
            current_input = initial_input
            st.subheader("Workflow Results:")
            for i, step in enumerate(st.session_state.prompt_workflow_steps):
                prompt = step.get('prompt', '').strip()
                if not prompt:
                    st.warning(f"Step {i+1} has no prompt. Skipping.")
                    continue

                st.markdown(f"---")
                st.markdown(f"**Running Step {i + 1}**")
                st.info(f"Input to Step {i+1}:")
                st.text(current_input[:500] + "..." if len(current_input) > 500 else current_input) # Show truncated input

                with st.spinner(f"Executing Step {i + 1}..."):
                    try:
                        # Combine the prompt and the current input
                        response = model.generate_content(f"{prompt}\n\nInput: {current_input}")
                        step_output = response.text
                        st.success(f"Output from Step {i + 1}:")
                        st.markdown(step_output) # Use markdown for better formatting
                        current_input = step_output # The output of this step is the input for the next
                    except Exception as e:
                        st.error(f"Error executing Step {i + 1}: {e}")
                        # Stop the workflow if an error occurs
                        break

else:
    st.warning("Please enter your Gemini API key in the sidebar.")
